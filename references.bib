@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B. and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{jia2019bias,
  title={Bias in Multilingual Models: The Case for Linguistic Equity in AI},
  author={Jia, Roger and others},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@article{conneau2020unsupervised,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Conneau, Alexis and others},
  journal={arXiv preprint arXiv:1911.02116},
  year={2020}
}

@article{smith2021can,
  title={Can Multilingual Models Transfer for Less Resourced Languages?},
  author={Smith, Linda and others},
  journal={Language Resources and Evaluation},
  year={2021}
}

@article{zhang2020improving,
  title={Improving Multilingual Models with Language-Clustered Vocabularies},
  author={Zhang, Yiming and others},
  journal={arXiv preprint arXiv:2007.07680},
  year={2020}
}

@article{wu2019google,
  title={Googleâ€™s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
  author={Wu, Yonghui and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2019}
}

@article{shazeer2017outrageously,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Shazeer, Noam and others},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@article{hendrycks2021measuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and others},
  journal={arXiv preprint arXiv:2009.03300},
  year={2021}
}


@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE},
  doi={10.1109/SOCPAR.2014.7008025}
}

@inproceedings{keshet2016prediction,
  title={Prediction-Based, Prioritized Market-Share Insight Extraction},
  author={Keshet, Renato and Maor, Alina and Kour, George},
  booktitle={Advanced Data Mining and Applications: 12th International Conference, ADMA 2016, Gold Coast, QLD, Australia, December 12-15, 2016, Proceedings 12},
  pages={81--94},
  year={2016},
  organization={Springer}
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{lepikhin2020gshard,
      title={GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding}, 
      author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
      year={2020},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      journal = {arXiv preprint arXiv:2006.16668},
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{fedus2022review,
  title={A review of sparse expert models in deep learning},
  author={Fedus, William and Dean, Jeff and Zoph, Barret},
  journal={arXiv preprint arXiv:2209.01667},
  year={2022}
}

@inproceedings{clark2022unified,
  title={Unified scaling laws for routed language models},
  author={Clark, Aidan and De Las Casas, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and others},
  booktitle={International Conference on Machine Learning},
  pages={4057--4086},
  year={2022},
  organization={PMLR}
}

@article{hazimeh2021dselect,
  title={Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning},
  author={Hazimeh, Hussein and Zhao, Zhe and Chowdhery, Aakanksha and Sathiamoorthy, Maheswaran and Chen, Yihua and Mazumder, Rahul and Hong, Lichan and Chi, Ed},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29335--29347},
  year={2021}
}

@article{zhou2022mixture,
  title={Mixture-of-experts with expert choice routing},
  author={Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7103--7114},
  year={2022}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}


@article{kopf2023openassistant,
  title={OpenAssistant Conversations--Democratizing Large Language Model Alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={arXiv preprint arXiv:2304.07327},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{chiang2024chatbot,
  title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
  author={Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2403.04132},
  year={2024}
}

@article{lai2023okapi,
  title={Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback},
  author={Lai, Viet Dac and Van Nguyen, Chien and Ngo, Nghia Trung and Nguyen, Thuat and Dernoncourt, Franck and Rossi, Ryan A and Nguyen, Thien Huu},
  journal={arXiv preprint arXiv:2307.16039},
  year={2023}
}

@article{whitehouse2023llm,
  title={Llm-powered data augmentation for enhanced crosslingual performance},
  author={Whitehouse, Chenxi and Choudhury, Monojit and Aji, Alham Fikri},
  journal={arXiv preprint arXiv:2305.14288},
  year={2023}
}

@article{ustun2024aya,
  title={Aya model: An instruction finetuned open-access multilingual language model},
  author={{\"U}st{\"u}n, Ahmet and Aryabumi, Viraat and Yong, Zheng-Xin and Ko, Wei-Yin and D'souza, Daniel and Onilude, Gbemileke and Bhandari, Neel and Singh, Shivalika and Ooi, Hui-Lee and Kayid, Amr and others},
  journal={arXiv preprint arXiv:2402.07827},
  year={2024}
}

@article{costa2022no,
  title={No language left behind: Scaling human-centered machine translation},
  author={Costa-juss{\`a}, Marta R and Cross, James and {\c{C}}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and others},
  journal={arXiv preprint arXiv:2207.04672},
  year={2022}
}

@article{koehn2017six,
  title={Six challenges for neural machine translation},
  author={Koehn, Philipp and Knowles, Rebecca},
  journal={arXiv preprint arXiv:1706.03872},
  year={2017}
}

@article{son2023translation,
  title={Translation Performance from the User's Perspective of Large Language Models and Neural Machine Translation Systems},
  author={Son, Jungwoo and Kim, Byeongil},
  journal={Information},
  volume={14},
  number={10},
  pages={574},
  year={2023}
}

@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@article{dai2019transformer,
  title={Transformer-XL: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{nichols2008tutorial,
  title={Tutorial on computational linguistic phylogeny},
  author={Nichols, Johanna and Warnow, Tandy},
  journal={Language and Linguistics Compass},
  volume={2},
  number={5},
  pages={760--820},
  year={2008}
}

@book{birch2021neural,
  title={Neural Machine Translation},
  author={Birch, Alexandra},
  year={2021},
  publisher={Cambridge University Press}
}

@article{seamlessm4t2023,
  title={Introducing SeamlessM4T, a Multimodal AI Model for Speech Translation},
  author={Meta AI},
  journal={Meta AI Blog},
  year={2023}
}


@article{zoph2022designing,
  title={Designing effective sparse expert models},
  author={Zoph, Barret},
  journal={IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  year={2022}
}

@article{ott2022opt,
  title={OPT: Open pre-trained transformer language models},
  author={Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, P Sai and Sridhar, Abhinav and Wang, Tao and Zettlemoyer, Luke},
  year={2022}
}

@inproceedings{zheng2019chid,
  title={CHID: A large-scale Chinese idiom dataset for cloze test},
  author={Zheng, Chujie and Huang, Minlie and Sun, Aixin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={778--787},
  year={2019},
  doi={10.18653/v1/P19-1075},
  url={https://doi.org/10.18653/v1/p19-1075}
}

@article{ahuja2023mega,
  title={Mega: Multilingual evaluation of generative ai},
  author={Ahuja, Kabir and Diddee, Harshita and Hada, Rishav and Ochieng, Millicent and Ramesh, Krithika and Jain, Prachi and Nambi, Akshay and Ganu, Tanuja and Segal, Sameer and Axmed, Maxamed and others},
  journal={arXiv preprint arXiv:2303.12528},
  year={2023}
}

@misc{vu2023freshllms,
      title={FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation}, 
      author={Tu Vu and Mohit Iyyer and Xuezhi Wang and Noah Constant and Jerry Wei and Jason Wei and Chris Tar and Yun-Hsuan Sung and Denny Zhou and Quoc Le and Thang Luong},
      year={2023},
      eprint={2310.03214},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{press2022measuring,
  title={Measuring and narrowing the compositionality gap in language models},
  author={Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2210.03350},
  year={2022}
}

@article{komeili2021internet,
  title={Internet-augmented dialogue generation},
  author={Komeili, Mojtaba and Shuster, Kurt and Weston, Jason},
  journal={arXiv preprint arXiv:2107.07566},
  year={2021}
}

@article{gala2024airavata,
  title   = {Airavata: Introducing Hindi Instruction-tuned LLM},
  author  = {Jay Gala and Thanmay Jayakumar and Jaavid Aktar Husain and Aswanth Kumar M and Mohammed Safi Ur Rahman Khan and Diptesh Kanojia and Ratish Puduppully and Mitesh M. Khapra and Raj Dabre and Rudra Murthy and Anoop Kunchukuttan},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2401.15006}
}

@article{sengupta2023jais,
  title={Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models},
  author={Sengupta, Neha and Sahu, Sunil Kumar and Jia, Bokang and Katipomu, Satheesh and Li, Haonan and Koto, Fajri and Afzal, Osama Mohammed and Kamboj, Samta and Pandit, Onkar and Pal, Rahul and others},
  journal={arXiv preprint arXiv:2308.16149},
  year={2023}
}

@article{group2024rakutenai,
  title={RakutenAI-7B: Extending Large Language Models for Japanese},
  author={Group, Rakuten and Levine, Aaron and Huang, Connie and Wang, Chenguang and Batista, Eduardo and Szymanska, Ewa and Ding, Hongyi and Chou, Hou Wei and Pessiot, Jean-Fran{\c{c}}ois and Effendi, Johanes and others},
  journal={arXiv preprint arXiv:2403.15484},
  year={2024}
}

@article{son2024kmmlu,
  title={KMMLU: Measuring Massive Multitask Language Understanding in Korean},
  author={Son, Guijin and Lee, Hanwool and Kim, Sungdong and Kim, Seungone and Muennighoff, Niklas and Choi, Taekyoon and Park, Cheonbok and Yoo, Kang Min and Biderman, Stella},
  journal={arXiv preprint arXiv:2402.11548},
  year={2024}
}